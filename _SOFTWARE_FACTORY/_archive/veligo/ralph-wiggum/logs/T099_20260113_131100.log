RLM Light Agent - Task: Tu es un agent TDD autonome pour Veligo (v√©los en libre-service).

## T√ÇCHE
Task T099: Test RLM Light

## D√âTAILS
# Task T099: Test RLM Light

STATUS: IN_PROGRESS

## Description
Test du RLM Light Client avec une t√¢che simple.

## Task
1. Utilise grep pour chercher "USE_RLM_LIGHT" dans tools/ralph-wiggum/
2. Montre les fichiers trouv√©s
3. Termine avec SUCCESS

## üî¥ R√àGLE ABSOLUE: V√âRIFIER AVANT MODIFIER

AVANT de modifier ou cr√©er un fichier, tu DOIS:
1. Utiliser `Read` pour V√âRIFIER que le fichier existe
2. Utiliser `veligo_grep` pour trouver les patterns existants
3. NE JAMAIS inventer un fichier qui n'existe pas

Si un fichier n'existe pas:
- Cherche un fichier similaire avec `Glob` ou `veligo_rag_query`
- Adapte ton approche au code R√âEL

## INSTRUCTIONS TDD (Red ‚Üí Green)

1. **PHASE EXPLORATION**: Comprendre le contexte
   - `veligo_rag_query("test skip failing")` pour trouver tests probl√©matiques
   - `veligo_grep("TODO|FIXME", "*.rs")` pour trouver points d'attention
   - `Read` sur les fichiers mentionn√©s pour V√âRIFIER qu'ils existent

2. **PHASE RED**: Lis le test qui √©choue
   - Utilise `Read` pour voir le fichier test (V√âRIFIE qu'il existe!)
   - Comprends ce que le test attend

3. **PHASE CODE**: Impl√©mente la solution
   - Utilise `Read` AVANT d'utiliser `Edit`
   - Modifie max 3 fichiers
   - Fais des changements minimaux

4. **PHASE GREEN**: V√©rifie
   - Rust: `cargo test --lib`
   - Frontend: `npm run test`
   - E2E: `npx playwright test <file>`

## R√àGLES ANTI-HALLUCINATION
- ‚ùå JAMAIS modifier un fichier sans l'avoir lu d'abord
- ‚ùå JAMAIS inventer un chemin de fichier
- ‚ùå JAMAIS pr√©tendre "tests pass" sans ex√©cuter la commande
- ‚úÖ Utilise veligo_grep pour trouver le bon fichier
- ‚úÖ Utilise Read pour v√©rifier l'existence

## OUTILS MCP DISPONIBLES
- veligo_rag_query: Recherche s√©mantique dans le code
- veligo_ao_search: Recherche dans les specs AO
- veligo_grep: Grep pattern dans fichiers

Commence maintenant. R√©ponds avec: SUCCESS ou NEEDS_MORE_WORK
API: http://127.0.0.1:8002/v1/chat/completions
Model: qwen3

--- Iteration 1/20 ---
LLM: {"tool": "veligo_grep", "pattern": "USE_RLM_LIGHT", "path": "tools/ralph-wiggum/**"}...
TOOL [veligo_grep]: ERROR: Unknown tool: veligo_grep...

--- Iteration 2/20 ---
LLM: {"tool": "grep", "pattern": "USE_RLM_LIGHT", "path": "tools/ralph-wiggum/**"}...
TOOL [grep]: No matches found...

--- Iteration 3/20 ---
LLM: {"tool": "grep", "pattern": "USE_RLM_LIGHT", "path": "tools/ralph-wiggum/*"}...
TOOL [grep]: No matches found...

--- Iteration 4/20 ---
LLM: {"tool": "grep", "pattern": "USE_RLM_LIGHT", "path": "tools/ralph-wiggum/**/*.rs"}...
TOOL [grep]: No matches found...

--- Iteration 5/20 ---
LLM: {"tool": "read", "path": "tools/ralph-wiggum/"}...
TOOL [read]: ERROR: [Errno 21] Is a directory: '/Users/sylvain/_LAPOSTE/_VELIGO2/tools/ralph-wiggum'...

--- Iteration 6/20 ---
LLM: {"tool": "bash", "command": "ls -R tools/ralph-wiggum/"}...
TOOL [bash]: exit_code=0
README.md
__pycache__
adversarial_agent.sh
agentic_worker.sh
audits
claude_agent_runner.expect
hello_world.py
llm_worker.sh
logs
lrm_brain.py
lrm_brain.sh
lrm_orchestrator.sh
ralph_full_pi...

--- Iteration 7/20 ---
LLM: {"tool": "read", "path": "tools/ralph-wiggum/rlm_light_client.py"}...
TOOL [read]: #!/usr/bin/env python3
"""
RLM Light Client - Agent minimaliste avec tools
Bypass opencode (40-60k overhead) en gardant les outils essentiels

Architecture:
1. API llama.cpp directe
2. Tools int√©gr√©s:...

=== RESULT ===
#!/usr/bin/env python3
"""
RLM Light Client - Agent minimaliste avec tools
Bypass opencode (40-60k overhead) en gardant les outils essentiels

Architecture:
1. API llama.cpp directe
2. Tools int√©gr√©s: read, edit, bash, grep
3. Agentic loop avec tool calling
4. ~2k tokens system prompt vs 40-60k pour opencode
"""

import json
import subprocess
import sys
import re
import os
from pathlib import Path
import requests
from typing import Optional, Dict, Any, List

# Configuration
LLAMA_API_URL = os.getenv("LLAMA_API_URL", "http://127.0.0.1:8002/v1/chat/completions")
LLAMA_MODEL = os.getenv("LLAMA_MODEL", "qwen3")
LLAMA_MAX_TOKENS = int(os.getenv("LLAMA_MAX_TOKENS", "8192"))
TIMEOUT = int(os.getenv("AGENT_TIMEOUT", "7200"))
PROJECT_ROOT = Path(__file__).parent.parent.parent  # /Users/sylvain/_LAPOSTE/_VELIGO2 (veligo root)

# System prompt minimaliste (~2k tokens)
SYSTEM_PROMPT = """Tu es un agent RLM (Read-Like-Modify). Tu as acc√®s √† ces outils:

## OUTILS DISPONIBLES

### read
Lit un fichier. Usage: {"tool": "read", "path": "<chemin>"}

### edit
Modifie un fichier (find/replace). Usage: {"tool": "edit", "path": "<chemin>", "old": "<texte √† remplacer>", "new": "<nouveau texte>"}

### bash
Ex√©cute une commande bash. Usage: {"tool": "bash", "command": "<commande>"}

### grep
Recherche dans les fichiers. Usage: {"tool": "grep", "pattern": "<regex>", "path": "<chemin ou glob>"}

### done
Termine la t√¢che. Usage: {"tool": "done", "result": "<r√©sum√©>"}

## R√àGLES RLM (Read-Like-Modify)

1. **VERIFY BEFORE MODIFY**: Toujours lire (read/grep) avant d'√©diter
2. **SMALL BATCHES**: Un seul changement √† la fois
3. **NO HALLUCINATION**: Ne jamais inventer de fichiers ou de code
4. **TEST AFTER CHANGE**: Toujours v√©rifier avec bash apr√®s modification

## FORMAT DE R√âPONSE

Pour utiliser un outil, r√©ponds EXACTEMENT avec ce format JSON sur une ligne:
{"tool": "<nom>", ...params}

Tu peux ajouter du texte explicatif AVANT l'appel d'outil, mais l'appel doit √™tre sur une ligne seule.
"""

def call_llm(messages: List[Dict[str, str]], max_tokens: int = LLAMA_MAX_TOKENS) -> Optional[str]:
    """Appel direct √† llama.cpp API"""
    payload = {
        "model": LLAMA_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.3,  # Plus d√©terministe
    }

    try:
        response = requests.post(
            LLAMA_API_URL,
            json=payload,
            timeout=TIMEOUT
        )
        response.raise_for_status()
        data = response.json()
        return data.get("choices", [{}])[0].get("message", {}).get("content", "")
    except Exception as e:
        print(f"ERROR: LLM call failed: {e}", file=sys.stderr)
        return None

def execute_tool(tool_call: Dict[str, Any]) -> str:
    """Ex√©cute un outil et retourne le r√©sultat"""
    tool = tool_call.get("tool", "")

    if tool == "read":
        path = tool_call.get("path", "")
        full_path = PROJECT_ROOT / path if not path.startswith("/") else Path(path)
        try:
            if not full_path.exists():
                return f"ERROR: File not found: {path}"
            content = full_path.read_text(encoding="utf-8")
            # Limiter √† 10k chars
            if len(content) > 10000:
                content = content[:10000] + "\n... (truncated)"
            return content
        except Exception as e:
            return f"ERROR: {e}"

    elif tool == "edit":
        path = tool_call.get("path", "")
        old = tool_call.get("old", "")
        new = tool_call.get("new", "")
        full_path = PROJECT_ROOT / path if not path.startswith("/") else Path(path)
        try:
            if not full_path.exists():
                return f"ERROR: File not found: {path}"
            content = full_path.read_text(encoding="utf-8")
            if old not in content:
                return f"ERROR: Old text not found in file"
            new_content = content.replace(old, new, 1)
            full_path.write_text(new_content, encoding="utf-8")
            return f"OK: Edited {path}"
        except Exception as e:
            return f"ERROR: {e}"

    elif tool == "bash":
        command = tool_call.get("command", "")
        try:
            result = subprocess.run(
                command,
                shell=True,
                cwd=str(PROJECT_ROOT),
                capture_output=True,
                text=True,
                timeout=300
            )
            output = result.stdout + result.stderr
            # Limiter √† 5k chars
            if len(output) > 5000:
                output = output[:5000] + "\n... (truncated)"
            return f"exit_code={result.returncode}\n{output}"
        except subprocess.TimeoutExpired:
            return "ERROR: Command timed out (300s)"
        except Exception as e:
            return f"ERROR: {e}"

    elif tool == "grep":
        pattern = tool_call.get("pattern", "")
        path = tool_call.get("path", ".")
        try:
            result = subprocess.run(
                ["rg", "-n", "--max-count=50", pattern, path],
                cwd=str(PROJECT_ROOT),
                capture_output=True,
                text=True,
                timeout=60
            )
            output = result.stdout
            if len(output) > 5000:
                output = output[:5000] + "\n... (truncated)"
            return output or "No matches found"
        except Exception as e:
            return f"ERROR: {e}"

    elif tool == "done":
        return f"{tool_call.get('result', 'Done')}"

    else:
        return f"ERROR: Unknown tool: {tool}"

def extract_tool_call(text: str) -> Optional[Dict[str, Any]]:
    """Extrait l'appel d'outil JSON de la r√©ponse"""
    # Cherche une ligne JSON avec "tool"
    for line in text.split("\n"):
        line = line.strip()
        if line.startswith("{") and '"tool"' in line:
            try:
                return json.loads(line)
            except json.JSONDecodeError:
                continue
    return None

def run_agent(task: str, max_iterations: int = 20) -> str:
    """Boucle agentique RLM Light"""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": task}
    ]

    for iteration in range(max_iterations):
        print(f"\n--- Iteration {iteration + 1}/{max_iterations} ---", file=sys.stderr)

        # Appel LLM
        response = call_llm(messages)
        if not response:
            return "ERROR: LLM call failed"

        print(f"LLM: {response[:500]}...", file=sys.stderr)

        # Ajoute la r√©ponse √† l'historique
        messages.append({"role": "assistant", "content": response})

        # Extrait l'appel d'outil
        tool_call = extract_tool_call(response)

        if not tool_call:
            # Pas d'outil = r√©ponse finale (pas id√©al mais on continue)
            print("WARNING: No tool call found, asking for completion", file=sys.stderr)
            messages.append({"role": "user", "content": "Tu dois utiliser un outil. Si tu as termin√©, utilise: {\"tool\": \"done\", \"result\": \"<r√©sum√©>\"}"})
            continue

        # Ex√©cute l'outil
        tool_result = execute_tool(tool_call)
        print(f"TOOL [{tool_call.get('tool')}]: {tool_result[:200]}...", file=sys.stderr)

        # V√©rifie si termin√©
        if "TASK_COMPLETE:" in tool_result:
            return tool_result.replace("", "")

        # Ajoute le r√©sultat √† l'historique
        messages.append({"role": "user", "content": f"R√©sultat de l'outil:\n{tool_result}"})

    return "ERROR: Max iterations reached without completion"

def main():
    if len(sys.argv) < 2:
        print("Usage: rlm_light_client.py '<task description>'", file=sys.stderr)
        print("\nExemple: rlm_light_client.py 'Corrige le test auth.spec.ts qui √©choue'", file=sys.stderr)
        sys.exit(1)

    task = " ".join(sys.argv[1:])
    print(f"RLM Light Agent - Task: {task}", file=sys.stderr)
    print(f"API: {LLAMA_API_URL}", file=sys.stderr)
    print(f"Model: {LLAMA_MODEL}", file=sys.stderr)

    result = run_agent(task)
    print(f"\n=== RESULT ===\n{result}")

if __name__ == "__main__":
    main()
