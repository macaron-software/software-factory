# Macaron Software â€” Project Context

## REPO STRUCTURE
```
_FACTORY_CORE/          # Shared utils (subprocess, logging, LLM, daemon)
_SOFTWARE_FACTORY/      # Software Factory (SF) â€” TDD automation + Agent Platform
  â”œâ”€â”€ cli/factory.py    # CLI: factory <project> <command>
  â”œâ”€â”€ core/             # Brain, TDD workers, adversarial, FRACTAL
  â”œâ”€â”€ platform/         # Software Factory â€” FastAPI web app
  â”‚   â”œâ”€â”€ server.py     # App factory, port 8090 (public) / 8099 (dev)
  â”‚   â”œâ”€â”€ web/routes/   # HTTP routes package (10 sub-modules, was 8800 LOC monolith)
  â”‚   â”œâ”€â”€ a2a/          # Agent-to-Agent messaging (bus, negotiation, veto)
  â”‚   â”œâ”€â”€ agents/       # Loop, executor, store (143 agents)
  â”‚   â”œâ”€â”€ patterns/     # 8 orchestration patterns
  â”‚   â”œâ”€â”€ missions/     # SAFe mission lifecycle + ProductBacklog (features, stories)
  â”‚   â”œâ”€â”€ llm/          # Multi-provider LLM client
  â”‚   â”œâ”€â”€ tools/        # Agent tools (code, git, deploy, memory, security, browser)
  â”‚   â”œâ”€â”€ ops/          # Auto-heal, chaos endurance, endurance watchdog
  â”‚   â”œâ”€â”€ services/     # Notification (Slack/Email/Webhook)
  â”‚   â””â”€â”€ deploy/       # Dockerfile + docker-compose (Azure VM)
  â”œâ”€â”€ Dockerfile        # Public Docker image (python:3.12-slim, non-root macaron)
  â”œâ”€â”€ docker-compose.yml # Public compose (port 8090, demo mode default)
  â”œâ”€â”€ Makefile          # setup/run/stop/logs/dev/test/clean
  â”œâ”€â”€ .env.example      # Template (PLATFORM_LLM_PROVIDER=demo by default)
  â”œâ”€â”€ projects/*.yaml   # Per-project configs
  â”œâ”€â”€ skills/*.md       # Domain-specific prompts
  â””â”€â”€ data/             # SQLite DBs (factory.db, platform.db)
_MIGRATION_FACTORY/     # Code migration engine (Angular 16â†’17, ISO 100%)
```

## PUBLIC REPO
```
GitHub:   macaron-software/software-factory (AGPL-3.0)
Tags:     v1.0.0, v1.1.0, v1.2.0
Clone:    /tmp/gh_push_ops/software-factory (for pushes, auth: leglands via gh)
README:   8 languages (EN/FR/ZH/ES/JA/PT/DE/KO)
Docker:   git clone â†’ make setup â†’ make run â†’ http://localhost:8090
Demo:     PLATFORM_LLM_PROVIDER=demo (mock LLM, no API key needed)
```

## RUN COMMANDS
```bash
# SF CLI
cd _SOFTWARE_FACTORY && source setup_env.sh
factory <p> brain run --mode vision|fix|security|refactor|missing
factory <p> cycle start -w 5 -b 20 -t 30   # batch TDD
factory status --all

# Platform â€” local dev (NEVER --reload, ALWAYS --ws none)
python3 -m uvicorn platform.server:app --host 0.0.0.0 --port 8099 --ws none --log-level warning

# Platform â€” public Docker
cd software-factory && make setup && make run  # â†’ http://localhost:8090

# Tests
python3 -m pytest tests/ -v                     # 52 tests
cd platform/tests/e2e && npx playwright test    # 82 tests (9 specs)
```

## LLM PROVIDERS (env-driven)
```
Local (default):  PLATFORM_LLM_PROVIDER=minimax    PLATFORM_LLM_MODEL=MiniMax-M2.5
Azure (docker):   PLATFORM_LLM_PROVIDER=azure-openai PLATFORM_LLM_MODEL=gpt-5-mini
```
Fallback chain: primary â†’ next in [minimax, azure-openai, azure-ai]
- MiniMax M2.5: fast, cheap, <think> blocks consume tokens (min 16K)
- Azure GPT-5-mini: reasoning model, NO temperature (only 1.0), needs max_completion_tokensâ‰¥8K
- Azure GPT-5.2: swedencentral private endpoint (VNet only)
- Keys: `~/.config/factory/*.key` â€” NEVER set `*_API_KEY=dummy`
- Client: `platform/llm/client.py` â€” `_PROVIDERS`, `_FALLBACK_CHAIN`, cooldown 90s on 429

## AZURE INFRASTRUCTURE
```
VM:   vm-macaron (RG-MACARON, francecentral) â€” Standard_D4as_v5 (4 vCPU, 16GB)
      IP: 4.233.64.30, SSH: azureadmin, nginx basic auth: macaron/macaron
      Files: /opt/macaron/platform/ â€” docker compose (platform + nginx)
      NOTE: D4as_v6 fails (SCSI disk controller incompatible, needs NVMe)
      NOTE: active compose = deploy/docker-compose-vm.yml (deploy project)
      NOTE: /opt/macaron/platform/docker-compose.yml = LEGACY, must stay stopped
      NOTE: after rsync: ALWAYS `cd /opt/macaron/platform && docker compose down`
      NOTE: files owned by azureadmin â†’ SCP to /tmp + sudo cp

PG:   macaron-platform-pg (RG-MACARON, francecentral) â€” B1ms, PG 17, 32GB
      FQDN: macaron-platform-pg.postgres.database.azure.com
      DB: macaron_platform | User: macaron | SSL: require
      Extensions: pgvector 0.8.0, pg_trgm 1.6, uuid-ossp 1.1
      Firewall: allow-vm (4.233.64.30), allow-dev (update IP as needed)
      Status: ACTIVE â€” dual adapter (adapter.py), data migrated
      Schema: schema_pg.sql (33 tables, tsvector FTS, pgvector embeddings)
      Adapter: platform/db/adapter.py â€” translates SQLite SQLâ†’PG SQL transparently
      Migration: platform/db/migrate_data.py â€” batch SQLiteâ†’PG with ON CONFLICT DO NOTHING

LLM:  ascii-ui-openai (rg-ascii-ui, francecentral) â€” gpt-5-mini
      Capacity: 100 req/min, 100K tokens/min
      NOTE: castudioia* resources are private endpoint (VNet only, unusable)

Deploy: rsync to /tmp + sudo cp (permissions). Or hotfix: cat file | ssh vm-macaron "docker exec -i CID tee /app/macaron_platform/PATH"
        Package installed as macaron_platform (NOT platform) â€” container path: /app/macaron_platform/
        After hotfix: clear pyc (`find /app/macaron_platform -name "*.pyc" -delete`) + restart container
        VM resize: az vm deallocate â†’ az vm resize â†’ az vm start
        Container rebuild: cd /opt/macaron/platform/deploy && sudo docker compose up -d --build
        Auto-resume: server.py lifespan (L98-118) restarts "running" missions on container restart
        Volume: deploy_platform-data at /app/data â€” survives container recreation
        âš ï¸ No git remote, no CI/CD â€” hotfixes lost on rebuild (persistence plan: Phase 1-5 above)

DR:   L3 full â€” 14/14 checks verified, 100% coverage
      Blob: macaronbackups (Standard_GRS francecentralâ†’francesouth, secondary=available)
      Containers: db-backups/ pg-dumps/ secrets/ (lifecycle: daily=90d, weekly=365d)
      Snapshots: vm-macaron-snap-* (incremental, keep 4)
      PG PITR: 7-day native (no geo-redundant on Burstable â†’ compensated by blob GRS)

      Commands (always run from /tmp or via run_*.py â€” avoid 'platform' package shadowing):
        python3 platform/ops/run_backup.py [--tier daily|weekly] [--pg-only|--sqlite-only|--secrets-only]
        python3 platform/ops/run_restore.py --list
        python3 platform/ops/run_restore.py --latest --dry-run
        python3 platform/ops/run_restore.py --latest [--pg-only|--sqlite-only|--secrets-only]
        python3 platform/ops/run_restore.py --from-snapshot vm-macaron-snap-YYYYMMDD
        python3 platform/ops/run_health.py [--watch] [--json]

      Backup contents:
        SQLite: 7 DBs (platform/factory/build_queue/metrics/project_context/rlm_cache/permissions_audit)
        PG: 33 tables, ~1085 rows, psycopg dump (SELECTâ†’INSERT ON CONFLICT DO NOTHING)
        Secrets: 5 API keys (.key) + .env + docker-compose.yml â†’ tar.gz
        VM: 30GB disk incremental snapshot

      Health monitor (5 checks): vm_http, pg_connectivity, vm_containers, vm_disk, backup_freshness

      RTO/RPO:
        PG:      RPO 24h (dump) + 7d PITR | RTO 15min
        SQLite:  RPO 24h                   | RTO 5min
        VM:      RPO 7d (weekly snapshot)  | RTO 30min
        Secrets: RPO 24h                   | RTO 5min
        Code:    RPO 0 (git)               | RTO 0

      Cron (local Mac):
        0 3 * * * cd /tmp && python3 /.../run_backup.py >> /tmp/macaron-backup.log 2>&1
        0 2 * * 0 cd /tmp && python3 /.../run_backup.py --tier weekly >> /tmp/macaron-backup.log 2>&1

      Runbook: platform/ops/RUNBOOK.md (5 scenarios: PG corruption, SQLite loss, VM loss, keys lost, full disaster)
```

## INFRA HARDENING (implemented)
```
Security:
  CORS: CORSMiddleware (localhost + VM origins)
  Headers: X-Content-Type-Options, X-Frame-Options, X-XSS-Protection, HSTS, Referrer-Policy
  Secrets: externalized â†’ ~/.config/factory/.env (chmod 600), no hardcoded defaults
  Docker: non-root user 'macaron' in both Dockerfiles
  Auth: AuthMiddleware (bearer token, MACARON_API_KEY env var)

Resilience:
  PG pool: psycopg_pool.ConnectionPool (min=2, max=20, idle=300s)
  LLM circuit breaker: 5 failures/60s â†’ open 120s, half-open probe, auto-recovery
  Bus: async persist via call_soon (non-blocking publish)
  Rate limit: PG-backed (rate_limit_hits table), per-IP+token, survives restart

Observability:
  Structured logging: JSON format, trace_id (per-request), agent_id, secret redaction
  LOG_LEVEL env var, LOG_FORMAT=text for human-readable
  OpenTelemetry: FastAPIInstrumentor + ConsoleSpanExporter (OTEL_ENABLED=1)
  Trace-ID: X-Trace-ID header (generated per-request, propagated in responses)

Auto-Heal (platform/ops/auto_heal.py):
  Pipeline: incident â†’ group â†’ TMA epic â†’ workflow (diagnoseâ†’fixâ†’verifyâ†’close)
  Loop: 60s scan, max 3 concurrent heals, severityâ‰¥P3
  Workflow: tma-autoheal (4 phases: hierarchicalâ†’adversarial-pairâ†’sequentialâ†’solo)
  Agents: Brain + Architect (diag), Senior Dev + QA (fix), SRE (verify), RTE (close)
  API: GET /api/autoheal/stats, POST /api/autoheal/trigger
  Env: AUTOHEAL_ENABLED=1, AUTOHEAL_INTERVAL=60, AUTOHEAL_SEVERITY=P3, AUTOHEAL_MAX_CONCURRENT=3
  Dedup: links new incidents to existing active TMA epics (no duplicates)
  Resolution: mission completed â†’ incidents auto-resolved

  âš ï¸ PERSISTENCE GAP (PLANNED â€” not yet implemented):
    Current: fixes via docker cp â†’ LOST on rebuild. No git remote, no CI/CD.
    Target: Auto-Heal â†’ git commit â†’ git push â†’ GitHub â†’ CI/CD (GH Actions) â†’ Docker build â†’ deploy Azure VM
    Plan:
      1. GitHub repo + secrets (SSH key, API keys)
      2. GH Actions: deploy.yml (buildâ†’GHCRâ†’SSH deployâ†’health checkâ†’rollback)
      3. GH Actions: pr-check.yml (lint+test, auto-merge heal PRs P0/P1)
      4. New tool git_push (approval required) in tools/git_tools.py
      5. auto_heal.py: after fix verified â†’ commit + push + PR auto
      6. Azure VM: deploy key SSH, container git clone at startup
      7. Circuit breaker: max 3 fails â†’ stop, revert auto si health check fail
    Safety: agents never touch secrets (CI/CD injects), main protected, 1 deploy/10min max
```

## SF PIPELINE
```
Brain (Opus) â†’ FRACTAL (3 concerns) â†’ TDD Workers (//) â†’ Adversarial â†’ Build â†’ Infra Check â†’ Deploy â†’ E2E â†’ Promote/Rollback â†’ Feedback â†’ XP Agent
```

## ADVERSARIAL (Team of Rivals â€” arXiv:2601.14351)
```
L0: deterministic (test.skip, @ts-ignore, empty catch) â†’ VETO ABSOLU
L1: LLM semantic (SLOP, hallucination, logic) â†’ VETO ABSOLU
  - HALLUCINATION/SLOP keywords â†’ force scoreâ‰¥7 + REJECT (engine.py has_critical_flags)
  - Retry loop: 5 attempts max, then NodeStatus.FAILED
L2: architecture (RBAC, validation, API design) â†’ VETO + ESCALATION
```
Multi-vendor: Brain=Opus, Worker=MiniMax, Security=GLM, Arch=Opus
Rule: "Code writers cannot declare their own success"

### MÃ©triques (core/metrics.py)

| MÃ©trique | Target | Source Paper |
|----------|--------|--------------|
| L0 catch rate | 25% | 24.9% (paper) |
| L1 catch rate | 75% | 87.8% (Code+Chart) |
| L2 catch rate | 85% | 14.6% supplÃ©mentaire |
| Final success | 90%+ | 92.1% (paper) |

Config: `projects/*.yaml` â†’ `adversarial:` block (cascade_enabled, l0_fast, l1_code, l2_arch, metrics_enabled)

## CoVe (Chain-of-Verification) â€” arxiv:2309.11495
4-stage anti-hallucination: Draft â†’ Plan Verify â†’ Answer Independently â†’ Final
Applied to Brain (featureâ†’AO trace), Adversarial (bypass detection), Infra (diagnostic)

## AO TRACEABILITY
Rule: no feature without AO ref (= SLOP). Config: `ao_compliance.enabled/refs_file` in projects/*.yaml

## DEPLOY & POST-DEPLOY
- CLI tools per project: `factory <p> deploy staging|prod`
- All stages gated by adversarial (build, staging, E2E, prod)
- Stuck detection: 0 chars 5min â†’ fallback LLM chain
- Post-deploy: TMC (k6) â†’ Chaos Monkey â†’ TMC Verify â†’ DEPLOYED
  - TMC: p95<500ms, errors<1%, >50rps. Fail â†’ perf task
  - Chaos: kill/latency/cpu scenarios, 30s recovery. Fail â†’ ROLLBACK
  - Files: `core/tmc_runner.py`, `core/chaos_runner.py`

## LEAN/XP
- WIP limits: max_concurrent, max_per_domain, wsjf queue priority
- WSJF dynamique: recalcul on feedback, decay temporel
- Quality gates: coverage 80%+, complexity <15, security 0 critical
- Canary deploy: 1%â†’10%â†’50%â†’100%, rollback si error>baseline+5%
- XP Agent retro: weekly patterns â†’ auto-patch adversarial rules

## MCP ARCHITECTURE
```
MCP LRM (port 9500) â† proxy (stdio) â† opencode workers
MCP Platform (port 9501, auto-start) â† proxy â† agents/CLI
```
LRM tools: lrm_locate/read/conventions/task_read/task_update/build, confluence_search/read, jira_search
Platform tools: platform_agents/missions/phases/messages/memory/git/code/metrics
Anonymization: PII stripping (phones, emails, names) via `mcp_lrm/anonymizer.py`
RLM Cache: SQLite FTS5 `data/rlm_cache.db`, 1h TTL, cache-first

## BUILD QUEUE & CYCLE WORKER
Global build queue: 1 job at a time, all projects. `factory queue start/stop/status`
Cycle worker (preferred): batch N TDD â†’ 1 build â†’ deploy. `factory <p> cycle start -w 5 -b 20 -t 30`
Files: `core/build_queue.py`, `core/cycle_worker.py`

## CORE MODULES
- **Brain** (`core/brain.py`): recursive analysis, modes: all/vision/fix/security/perf/refactor/test/migrate/debt/missing
  - Missing mode: TRUE TDD â€” finds tests importing non-existent modules â†’ IMPLEMENT tasks
  - Context: ProjectContext RAG (10 categories, FTS5, auto-refresh 1h)
- **Cycle** (`core/cycle_worker.py`): TDDâ†’BUILDâ†’DEPLOY, batch, no FRACTAL
- **Wiggum** (`core/wiggum_tdd.py`): pool daemon, FRACTAL enabled, adversarial per commit
- **Skills** (`core/skills.py`): domainâ†’prompt auto-mapping (smoke_ihm, e2e_api, e2e_ihm, ui, ux, tdd)
- **FRACTAL** (`core/fractal.py`): L1=3 concerns (featureâ†’guardsâ†’failures), L2=KISS atomic (implâ†’testâ†’verify)
- **Adversarial** (`core/adversarial.py`): 100% LLM+CoVe, zero regex. Understands context (CLI Exitâ‰ skip, fixture secretsâ‰ leak)
- **Infra** (`core/wiggum_infra.py`): check_site/docker/nginx/db â†’ CoVe diagnosis â†’ auto-fix
- **Deploy** (`core/wiggum_deploy.py`): BUILDâ†’ADVERSARIALâ†’INFRA CHECKâ†’STAGINGâ†’E2E(subprocess direct)â†’PROD
- **TaskStore** (`core/task_store.py`): SQLite data/factory.db, status: pendingâ†’lockedâ†’tdd_in_progressâ†’code_writtenâ†’buildâ†’commitâ†’deploy
- **ProjectContext** (`core/project_context.py`): RAG 10 categories, FTS5, auto-refresh 1h, 12K chars max
- **Meta-Awareness** (`core/meta_awareness.py`): cross-project error detection (50+ reps â†’ SYSTEMIC, 2+ projects â†’ CROSS-PROJECT)

## CLI REFERENCE
```bash
factory <p> brain run --mode vision|fix|security|perf|refactor|test|migrate|debt|missing
factory <p> brain --chat "q"
factory <p> cycle start -w 5 -b 20 -t 30   # PREFERRED (batch build)
factory <p> wiggum start -w 5               # LEGACY (1-by-1 build)
factory <p> infra check|diagnose|fix
factory queue start|stop|status
factory meta status|analyze --create-tasks
factory xp analyze --apply
factory <p> tasks retry [-t pending] [-s tdd_failed]
factory status --all
```

## PROJECTS
ppz psy veligo yolonow fervenza solaris **factory** (self)
LLM: brain=Opus, wiggum/cycle=MiniMax-M2.5, fallbackâ†’M2.1â†’GLM-4.7-free, timeout=30min

## BRAIN PHASE CYCLE
```
PHASE 1: FEATURES (vision) â†’ TDD â†’ DEPLOY â†’ OK? â†’ PHASE 2: FIXES â†’ TDD â†’ DEPLOY â†’ OK? â†’ PHASE 3: REFACTOR â†’ LOOP
```
Rule: NO REFACTOR until FIXES deployed. NO FIXES until FEATURES deployed.
Config: `brain.current_phase` + `brain.vision_doc` in projects/*.yaml

## CONVENTIONS
- **â›” ZERO SKIP**: NEVER `--skip-*`, `test.skip()`, `@ts-ignore`, `#[ignore]` â€” FIX > SKIP
- **Adversarial 100% LLM**: always MiniMax semantic, never regex
- **Cycle > Wiggum**: batch build, ~20x less CPU
- SvelteKit: no `+` prefix test files, tests in `__tests__/` subfolder
- opencode: `permission.doom_loop: "allow"` mandatory (prevents stdin hang)
- Process cleanup: `start_new_session=True` + `os.killpg()` on timeout

## TESTS INFRASTRUCTURE
```
Backend:   python3 -m pytest tests/ -v  (52 tests: health, projects, agents, missions, i18n, security, search, memory, demo)
E2E:       cd platform/tests/e2e && npx playwright test  (82 tests, 9 specs: portfolio, pages, journeys, project-chat, agents, epic, ideation, i18n, migration)
Config:    pyproject.toml (asyncio_mode=auto), playwright.config.ts (baseURL=http://4.233.64.30, timeout=120s, chromium, domcontentloaded)
Fresh:     Docker from-scratch verified: cloneâ†’make setupâ†’make runâ†’health OKâ†’all pages 200â†’API CRUD OK
```

### ENDURANCE + CHAOS TEST PLAN (pending)
```
tests/test_endurance.py   â€” 15+ pytest: project lifecycle, phase progression, auto-resume, LLM stats, 24h poll
tests/test_chaos.py       â€” 12+ pytest: container restart, CPU stress, network latency, DB pressure, disk fill, 4h chaos loop
tests/conftest.py         â€” Shared fixtures: live_url, live_session, canvas_project_id. Markers: endurance, chaos, live
endurance.spec.ts         â€” 11 Playwright: monitoring, SSE, pages during mission, API latency, post-chaos recovery
chaos.spec.ts             â€” 11 Playwright: visual recovery, data integrity, stale state, chaos controls

Backend support (ops/):
  chaos_endurance.py      â€” Asyncio loop: random 2-6h â†’ pick scenario â†’ execute â†’ log MTTR. Table chaos_runs
  endurance_watchdog.py   â€” 60s loop: phase stall detect, zombie missions, disk, LLM health. Table endurance_metrics
  llm_usage table         â€” Hook chat() â†’ INSERT per call. Aggregation: cost/day, cost/phase, cost/agent

Test project: "Macaron Canvas" (Figma clone) â€” stack decided by agents, run 24/7 indefinitely
Target: VM Azure #2 (B2s, ~30â‚¬/mois) for generated app deployment
```

### SECURITY HARDENING (implemented)
```
XSS:          Jinja2 autoescaping (|e), CSP headers, X-Content-Type-Options
SQL injection: parameterized queries throughout (? placeholders, no f-strings)
Prompt injection: LLM input sanitization, adversarial L0+L1 guards
Auth:         AuthMiddleware bearer token (platform/security/__init__.py), MACARON_API_KEY env var
              Smart exclusions: HTML pages, /static, /health, /sse always public
              Public GET API endpoints accessible without auth
              Mutation endpoints require Bearer token when MACARON_API_KEY set
CSP:          connect-src 'self' (no wildcard), unsafe-inline needed for HTMX
Redaction:    /api/monitoring/live strips docker IDs, kernel, git branch when unauthenticated
Secrets:      externalized ~/.config/factory/*.key, chmod 600, no hardcoded defaults
Docker:       non-root user 'macaron', minimal image
Headers:      HSTS, X-Frame-Options DENY, X-XSS-Protection, Referrer-Policy strict
```

### PRODUCT MANAGEMENT (implemented)
```
Backlog:      Epic(mission) â†’ Feature â†’ UserStory hierarchy. product.html, missions/product.py
Tables:       features, user_stories, feature_deps, sprints (all in migrations.py)
WSJF:         4 components (business_value, time_criticality, risk_reduction, job_duration)
              Slider UI in product.html (modal) + mission_detail.html (inline)
              POST /api/missions/{id}/wsjf â†’ auto-compute CoD/JD
              MissionDef dataclass has 4 WSJF fields + wsjf_score

Creation UI:  Inline forms: "+ Feature" under epic, "+ Story" under feature
              POST /api/epics/{id}/features, POST /api/features/{id}/stories

Kanban:       SortableJS drag-drop in mission_detail (kanban columns)
              PATCH /api/tasks/{id}/status (cross-column drag)

Prioritization: SortableJS on features list (â‹®â‹® drag handle)
                PATCH /api/backlog/reorder {type, ids} â†’ priority update

Dependencies: feature_deps table (feature_id, depends_on, dep_type)
              POST/GET/DELETE /api/features/{id}/deps
              Visual: ğŸ”— icons + dep count badges in product.html

Sprint Planning: Collapsible panel in mission_detail
                 GET /api/sprints/{id}/available-stories (unassigned backlog)
                 POST /api/sprints/{id}/assign-stories {story_ids}
                 DELETE /api/sprints/{id}/stories/{id}

Charts:       Chart.js â€” velocity bar (actual vs planned), burndown line (ideal + remaining)
              Cycle time histogram in dora_dashboard (/api/metrics/cycle-time)
              Gantt timeline bars in product_line.html

Releases:     GET /api/releases/{project_id} â†’ completed features by epic
              Release notes panel in pi_board + dora_dashboard
```

### REST API (JSON + FORM)
```
Helper:       _parse_body(request) â†’ auto-detect Content-Type, parse JSON or form
              _is_json_request(request) â†’ True if JSON (choose JSON vs redirect response)
              Located in web/routes/helpers.py

All POST endpoints accept both application/json and multipart/form-data:
  POST /api/projects              â†’ create project (JSON returns redirect or {"ok":true})
  POST /api/missions              â†’ create mission (JSON returns {"ok":true,"mission":{id,name}})
  POST /api/missions/{id}/start   â†’ start mission
  POST /api/missions/{id}/wsjf    â†’ compute WSJF
  POST /api/missions/{id}/sprints â†’ create sprint
  POST /api/epics/{id}/features   â†’ create feature
  POST /api/features/{id}/stories â†’ create story
  POST /api/features/{id}/deps    â†’ add dependency
  PATCH /api/features/{id}        â†’ update feature (SP, status, AC, priority, name)
  PATCH /api/stories/{id}         â†’ update story (SP, status, sprint_id, title)
  PATCH /api/tasks/{id}/status    â†’ update task status (kanban drag)
  PATCH /api/backlog/reorder      â†’ reorder features/stories by priority
  GET /api/sprints/{id}/available-stories â†’ unassigned stories for sprint
  GET /api/features/{id}/deps     â†’ list dependencies
  GET /api/releases/{project_id}  â†’ release notes
  GET /api/metrics/cycle-time     â†’ cycle time distribution
  DELETE /api/features/{id}/deps/{dep} â†’ remove dependency
  DELETE /api/sprints/{id}/stories/{id} â†’ unassign story
```

### MISSION EXECUTION (live)
```
Semaphore:    asyncio.Semaphore(2) â€” 2 concurrent missions max (helpers.py)
Auto-resume:  server.py lifespan finds running/paused missions â†’ resume after 15s delay
Phase timeout: 600s (10 min), max 2 reloops on QA/deploy failure
API:          POST /api/missions (create), GET /api/missions (list+progress), POST /api/missions/{id}/run
Orchestrator: services/mission_orchestrator.py â€” 11 phases sequential with gates (all_approved, no_veto, always)
Sprint mgmt:  auto-creation, feature pull PO, review/retro, velocity tracking, error reloop
```

### DEPLOY TO AZURE VM
```
Azure VM#1:   4.233.64.30 (macaron@), container deploy-platform-1
SSH:          sshpass -p "$VM_PASS" ssh azureadmin@4.233.64.30
Workflow:     scp â†’ docker cp â†’ clear __pycache__ â†’ docker restart â†’ wait 15s â†’ health
Container:    deploy-platform-1 (CID: docker ps -q -f 'name=deploy-platform-1')
Path mapping: local platform/ â†’ container /app/macaron_platform/ (NOT /app/platform/)
Templates:    no restart needed (Jinja2 re-reads)
âš ï¸ SCP collision: files named store.py from different dirs overwrite each other â†’ use unique names
Push clone:   /tmp/gh_push_ops/software-factory (auth via `gh auth` as leglands)
Public repo:  macaron-software/software-factory (AGPL-3.0)
Tags:         v1.0.0, v1.1.0, v1.2.0
```

### DOCKER DEPLOY (PUBLIC)
```
From scratch: git clone â†’ make setup (.env from .env.example) â†’ make run â†’ http://localhost:8090
Demo mode:    PLATFORM_LLM_PROVIDER=demo (default, no API key needed)
LLM mode:     Set in .env: PLATFORM_LLM_PROVIDER=azure-openai, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, PLATFORM_LLM_MODEL
Verified:     143 agents loaded, 13 projects provisioned, all 11 pages 200 OK, API CRUD JSON works
```

## FIGMA MCP
Figma Desktop (127.0.0.1:3845) via `proxy_figma.py`, fallback `mcp.figma.com`
Tools: get_file, get_node, get_styles, get_selection

## MIGRATION FACTORY (`_MIGRATION_FACTORY/`)
ISO 100%: OLD === NEW (bit-Ã -bit). NO features, NO improvements, NO refactoring during migration.
Transform: PRE-VALIDATE(golden files) â†’ TRANSFORM(codemod>LLM) â†’ POST-VALIDATE â†’ COMPARE(3-layer) â†’ COMMIT/ROLLBACK
Comparative adversarial: L0=golden diff(0%), L1a=backward compat, L1b=RLM exhaustiveness(MCP), L2=breaking changes
Codemods: jscodeshift (NgModuleâ†’standalone, FormGroupâ†’typed, *ngIfâ†’@if)
CLI: `factory-migrate <p> brain|transform|validate|status`
Files: `core/transform_worker.py`, `core/comparative_adversarial.py`, `core/migration_state.py`

## MACARON AGENT PLATFORM (`platform/`)

### VISION
Real agentic orchestration â‰  workflow automation (n8n/LangFlow = RPA glorifiÃ©)
Team of Rivals: agents debate, veto, negotiate, delegate â€” not boxes with arrows

### ARCH
```
FastAPI + HTMX + Jinja2 + SSE | Dark purple theme | SQLite(local) / PostgreSQL(Azure)
AgentLoop (async task) â†â†’ MessageBus (per-agent queues) â†’ SSE â†’ Frontend
AgentExecutor â†’ LLM (Azure/MiniMax) â†’ tool calls â†’ route via bus
```
Nav: Projects â†’ Workflows â†’ Patterns â†’ Agents â†’ Skills â†’ Memory â†’ MCPs | Settings

### DUAL SSE (CRITICAL)
`_push_sse()` pushes to BOTH `_sse_queues` (runner.py) AND `bus._sse_listeners` (broadcast)
`bus.publish()` â†’ `bus._sse_listeners` via `_notify_sse()`
SSE endpoint `/sse/session/{id}` â†’ `bus.add_sse_listener()` + filter by session_id

### PATTERN ENGINE (patterns/engine.py)
8 patterns: solo, sequential, parallel, loop, hierarchical, network, router, aggregator, human-in-the-loop
`run_pattern(PatternDef, session_id, task)` â†’ `_execute_node()` â†’ agent LLM streaming
Adversarial guard in `_execute_node`: L0 fast â†’ L1 semantic â†’ retry 2x â†’ FAILED if still reject

### MISSION CONTROL
CDP orchestre 11 phases product lifecycle:
1.IdÃ©ation(network) 2.ComitÃ©(HITL) 3.Constitution(seq) 4.Archi(aggregator)
5.Sprints(hierarchical) 6.CI/CD(seq) 7.QA(loop) 8.Tests(parallel)
9.Deploy(HITL) 10.TMA Routage(router) 11.Correctif(loop)

### SAFe (Score ~7/10)
WSJF real calc, sprint auto-creation, feature pull PO, sprint review/retro, velocity tracking
Learning loop, I&A retrospective, gates GO/NOGO/PIVOT, error reloop (QA fail â†’ dev-sprint max 2x)
Build gate: preflight build + reloop max 5x. TDD enforcement in dev-sprint prompt.

### KEY FILES
```
server.py, models.py, config.py
llm/client.py (multi-provider), llm/observability.py (per-call tracing)
a2a/bus.py (MessageBus), a2a/protocol.py, a2a/negotiation.py, a2a/veto.py
agents/loop.py (AgentLoop), agents/executor.py (LLM+tools), agents/store.py (143 agents YAML)
patterns/engine.py (8 patterns), patterns/store.py (PatternDef/Run/NodeState)
missions/store.py (MissionRun/PhaseRun/Sprint), missions/product.py (backlog)
sessions/store.py (SessionDef+MessageDef), sessions/runner.py (_push_sse dual)
memory/manager.py (4 layers FTS5), memory/vectors.py (pgvector/numpy dual)
db/adapter.py (SQLite/PG dual), db/migrations.py, db/schema_pg.sql, db/migrate_data.py
metrics/dora.py (DORA + velocity), workflows/store.py (27 nodes, 34 edges)
skills/library.py (1200+ GitHub), skills/definitions/*.yaml (42 YAML agents)
tools/ (code, git, deploy, memory, phase, browser, android, compose)
web/routes/ (10 sub-modules + helpers.py), web/ws.py (SSE), web/templates/ (20+ templates)
  routes/helpers.py: _parse_body(), _is_json_request() â€” dual JSON/form support all POST endpoints
  routes/missions.py: 9 new PM endpoints (feature/story CRUD, backlog reorder, deps, sprint assign)
  routes/projects.py: project CRUD + chat, all accept JSON via _parse_body()
  routes/pages.py: product_page loads deps_map + WSJF fields for template
mcp_platform/ (port 9501, auto-start, 8 tools)
```

### DB PATH
`data/platform.db` (racine _SOFTWARE_FACTORY), PAS `platform/data/`
Dual backend: `DATABASE_URL` env var selects PG, absent = SQLite
âš ï¸ NEVER `rm -f data/platform.db` â€” persistent user data. `init_db()` handles migrations idempotently.
âš ï¸ NEVER set `*_API_KEY=dummy` â€” overrides real keys from `~/.config/factory/*.key`

### MONITORING
DORA: deploy freq, lead time, CFR, MTTR + velocity + 12-week sparklines
LLM: per-call tracing (provider, model, tokens, cost), `/api/llm/stats`, `/api/llm/traces`
Live: `/monitoring` SSE agents/messages/sessions
Cost: per-model pricing ($0.30-$15/1M tokens)

### DASHBOARD VIEWS
DSI/CTO (`/dsi`), MÃ©tier (`/metier`), Portefeuille (`/`), Projet Board (`/projects/{id}/board`)

**MÃ©tier tab** (`/metier`, htmx-loaded from portfolio):
  - LEAN/SAFe product-centric, 100% live data (zero mock)
  - Top: Flow KPIs (WIP, completed, failed, avg lead time) â€” from missions DB
  - Left: Value Stream â€” Epic pipeline cards with phase dots (âœ“/â–¶/pending), progress bar, lead time, clickâ†’Mission Control
  - Right: Flow Metrics (throughput %, agents ART, WIP limit), Agent Velocity (top 8 by real message count), Activity Heatmap (28j from message timestamps)
  - Workflow Catalog: SAFe workflows with run counts
  - Route: `pages.py metier_page()`, Template: `metier.html`
  - NOTE: enum comparison â€” use `_s(val)` helper (`.value` for enums, `str()` otherwise)

### IDEATION
`/ideation` â†’ 5 agents (BA+Archi+UX+SÃ©cu+PM) â†’ run_pattern(network) â†’ streaming debate â†’ Briefâ†’Analyseâ†’SynthÃ¨se
"CrÃ©er Epic" â†’ PO creates project+git
NOTE: Graph init uses readyState check (not DOMContentLoaded) for htmx compatibility

### MEMORY
4-layer: sessionâ†’patternâ†’projectâ†’global (FTS5 on SQLite, tsvector on PG)
Wiki-like `/memory`, confidence bars, auto-population from epic creation
Retrospectives â†’ LLM analyze â†’ lessons â†’ memory_global (recursive self-improvement)

### MOBILE EPICS (Azure-hosted)
```
Workflows: mobile-ios-epic (SwiftUI, 5 phases), mobile-android-epic (Kotlin/Compose, 5 phases)
Phases: archi â†’ network â†’ features â†’ tests â†’ integration
Agents: 10 mobile-specific YAMLs (mobile_archi, mobile_ios_lead, mobile_android_lead, mobile_ux, mobile_qa, etc.)
Status: missions re-created after DB re-seed, workspace files preserved on deploy_platform-data volume
iOS output: ~19 Swift files (MVVM, async/await, Combine, URLSession)
Android output: ~37 Kotlin files (Compose, Hilt DI, OkHttp, Coroutines)
Android builder: deploy/Dockerfile.android (JDK 17, SDK 34, cmdline-tools, emulator headless)
âš ï¸ Android code_write path bug: agents write with duplicated workspace prefix (needs fix in executor.py)
```

### HTMX PATTERNS (common pitfalls)
- `DOMContentLoaded` does NOT fire for htmx-injected content â†’ use `readyState` check:
  `if(document.readyState==='loading'){addEventListener('DOMContentLoaded',fn)}else{fn()}`
- htmx tabs: `hx-get="/route" hx-select=".main-area > *" hx-target="#tab-content" hx-push-url="/?tab=X"`
- Enumâ†’string in Jinja: use `_s(val)` helper (`val.value if hasattr(val,'value') else str(val)`)
- Template injection: `<script>` in htmx fragments executes but DOM events already fired
